Ok so it is a tricky situation, 

I have been working with the cDDPM model recently, as it has the best performance. 
I have gotton it all working and with the call:
`python run.py experiment=cDDPM/DDPM_cond_spark_2D model.cfg.pretrained_encoder=False` it is now training. 

However, it has almost 70 million paramaters and so takes forever to train. I dont know what fucked up resources they have but for each of the three experiments on the code (of which `DDPM_cond_spark_2D` is just one) the max epochs is around 1000, and 27 epochs on the machine gpu is about 6 hours so wtf. And this isn't even pretraining the encoder bit.

So, what I am currently doing is trying to run it for 5 epochs just to see where everything gets saved to so I can look at the anomaly maps and get my head around what they are and how that works. 

Going forward, I think what I have to do is:

- Run for less epochs to undestand this shit (i.e. 2 not 5 - why did I pick 5???)
- Run the mDDPM experiment instead and see how that trains
- or the pDDPM one

- Or ask for the weights for the cDDPM one. 

Another interesting thing is that I was having problems with the dataset (again) as what the model expects is some bullshit filenames for BraTS data that is not mentioned anywhere AND that data is actually missing. 

So, I need to just double check that it is the same, and then proceed with their data (as its complete). However it is still good to have the working pipeline as it means for other data I can still use it. 

so, next steps
	- try to find and undertand the anomaly maps from a 5-epoch (or 2 epoch) run of cDDPM
	- try running mDDPM to see if its more practical
	- if not then try to get weights from somewhere
	- then get used to generating anomalys
SO

- I HAVE TO DO
	- sort the data so I have a pristing version of mine (need to get brats from old) a pristine version of finns, and then the sort of mixture to actually use for now..
OK OK 

Something wierd is going on

NEXT TASKS:
is to basically check the data in the FROM GIT FILE
- apply my sexy data renaming to it
- compare against what is expected
- and compare against their data
- and compare against the SPLIT DATA

THEN
	- run the 3 epoch training for cDDPM
	- start running and debuggind mDDPM


OK SO
- cDDPM is running but it is too slow
Next steps
- run mDDPM
- its slow as fuck

SO
- run the good on for ages?
- 


RUNNIG DATA FINALLY WORKED:

GET CLAUDE TO EXPLAIN WTF ALL THIS MEANS

C:\Users\rd81\OneDrive - University of Sussex\Desktop\FROM_GIT\Conditioned-Diffusion-Models-UAD\src\train.py:29: UserWarning:
The version_base parameter is not specified.
Please specify a compatability version level, or None.
Will assume defaults for version 1.1
  @hydra.main(config_path='configs', config_name='config') # Hydra decorator
CONFIG
├── trainer
│   └── _target_: pytorch_lightning.Trainer
│       gpus: -1
│       min_epochs: 1
│       max_epochs: 3
│       log_every_n_steps: 5
│       precision: 16
│       num_sanity_val_steps: 0
│       check_val_every_n_epoch: 1
│       benchmark: true
│       overfit_batches: false
│
├── model
│   └── _target_: src.models.DDPM_2D.DDPM_2D
│       cfg:
│         name: DDPM_2D
│         imageDim:
│         - 192
│         - 192
│         - 100
│         rescaleFactor: 2
│         interRes:
│         - 8
│         - 8
│         - 5
│         cropMode: isotropic
│         spatialDims: 2D
│         resizedEvaluation: true
│         unet_dim: 128
│         dim_mults:
│         - 1
│         - 2
│         - 2
│         learned_variance: false
│         learned_sinusoidal_cond: false
│         loss: l1
│         lossStrategy: mean
│         lr: 0.0001
│         scheduleLR: false
│         patienceLR: 10
│         earlyStopping: false
│         patienceStopping: 50
│         saveOutputImages: true
│         evalSeg: true
│         pad: true
│         erodeBrainmask: true
│         medianFiltering: true
│         threshold: auto
│         mode: t2
│         test_timesteps: 500
│         backbone: Spark_Encoder_2D
│         version: resnet50
│         cond_dim: 128
│         OpenaiUnet: true
│         spatial_transformer: false
│         condition: true
│         noisetype: simplex
│         encoder_path: xxx
│         pretrained_encoder: false
│         save_to_disc: false
│         noise_ensemble: true
│
├── datamodule
│   └── _target_: src.datamodules.Datamodules_train.IXI
│       cfg:
│         saveOutputImages: true
│         name: IXI
│         path:
│           pathBase: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED
│           IXI:
│             IDs:
│               train:
│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_train_fold│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_train_fold│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_train_fold│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_train_fold│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_train_fold│               val:
│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_val_fold0.│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_val_fold1.│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_val_fold2.│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_val_fold3.│               - /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_val_fold4.│               test: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/IXI_test.c│             keep_t2: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/avail_t2.│           Brats21:
│             IDs:
│               test: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/Brats21_te│               val: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/Brats21_val│           MSLUB:
│             IDs:
│               test: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/MSLUB_test│               val: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/splits/MSLUB_val.c│         imageDim:
│         - 192
│         - 192
│         - 100
│         rescaleFactor: 2
│         interRes:
│         - 8
│         - 8
│         - 5
│         cropMode: isotropic
│         spatialDims: 2D
│         unisotropic_sampling: true
│         sample_set: false
│         preLoad: true
│         curvatureFlow: true
│         percentile: true
│         pad: true
│         permute: false
│         randomRotate: false
│         rotateDegree: 5
│         horizontalFlip: false
│         randomBrightness: false
│         brightnessRange: (0.75,1.25)
│         randomContrast: false
│         contrastRange: (0.75,1.25)
│         modelpath: /Users/rd81/OneDrive - University of Sussex/Desktop/FROM_GIT/DATA_FULL_PROCESSED/Data/pretrained_2D│         num_workers: 16
│         batch_size: 32
│         lr: 0.0001
│         droplast: true
│         mode: t2
│         resizedEvaluation: true
│         testsets:
│         - Datamodules_eval.Brats21
│         - Datamodules_eval.MSLUB
│         - Datamodules_train.IXI
│         aug_intensity: true
│
├── callbacks
│   └── model_checkpoint:
│         _target_: pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint
│         monitor: val/Loss_comb
│         save_top_k: 1
│         auto_insert_metric_name: false
│         save_last: true
│         mode: min
│         dirpath: checkpoints/
│         filename: epoch-{epoch}_step-{step}_loss-{val/Loss_comb:.2f}
│
├── logger
│   └── wandb:
│         _target_: pytorch_lightning.loggers.wandb.WandbLogger
│         project: cDDPM
│         name: DDPM_2D_IXI_DDPM_cond_2D_spark_model.cfg.pretrained_encoder-False
│         save_dir: .
│         offline: false
│         id: null
│         resume: false
│         log_model: false
│         prefix: ''
│         job_type: ''
│         group: ''
│         tags: []
│
└── seed
    └── 3141
[2024-07-16 19:31:25,815][src.train][INFO] - Seed specified to 3141 by config
[2024-07-16 19:31:25,816][pytorch_lightning.utilities.seed][INFO] - Global seed set to 3141
[2024-07-16 19:31:25,819][src.train][INFO] - Training Fold 1 of 1 in the WandB group DDPM_cond_2D_spark
[2024-07-16 19:31:25,820][src.train][INFO] - Instantiating datamodule <src.datamodules.Datamodules_train.IXI>
[2024-07-16 19:31:26,663][src.train][INFO] - Instantiating model <src.models.DDPM_2D.DDPM_2D>
C:\ProgramData\Anaconda3\envs\my_env\lib\site-packages\monai\utils\tf32.py:66: UserWarning: torch.backends.cuda.matmul.allow_tf32 = True by default.
  This value defaults to True when PyTorch version in [1.7, 1.11] and may affect precision.
  See https://docs.monai.io/en/latest/precision_accelerating.html#precision-and-accelerating
  warnings.warn(
[sparse_cnn] model kwargs={'drop_path_rate': 0.05, 'pretrained': False, 'num_classes': 512}
[2024-07-16 19:31:29,213][src.train][INFO] - Instantiating callback <pytorch_lightning.callbacks.model_checkpoint.ModelCheckpoint>
[2024-07-16 19:31:29,215][src.train][INFO] - Instantiating logger <pytorch_lightning.loggers.wandb.WandbLogger>
wandb: Currently logged in as: raymondfdavey. Use `wandb login --relogin` to force relogin
wandb: wandb version 0.17.4 is available!  To upgrade, please run:
wandb:  $ pip install wandb --upgrade
wandb: Tracking run with wandb version 0.16.6
wandb: Run data is saved locally in .\wandb\run-20240716_193131-x2n9gg30
wandb: Run `wandb offline` to turn off syncing.
wandb: Syncing run DDPM_2D_IXI_DDPM_cond_2D_spark_model.cfg.pretrained_encoder-False
wandb:  View project at https://wandb.ai/raymondfdavey/cDDPM
wandb:  View run at https://wandb.ai/raymondfdavey/cDDPM/runs/x2n9gg30
[2024-07-16 19:31:39,294][src.train][INFO] - Instantiating trainer <pytorch_lightning.Trainer>
[2024-07-16 19:31:39,325][pytorch_lightning.utilities.rank_zero][INFO] - Using 16bit native Automatic Mixed Precision (AMP)
[2024-07-16 19:31:39,327][pytorch_lightning.utilities.rank_zero][INFO] - GPU available: True, used: True
[2024-07-16 19:31:39,327][pytorch_lightning.utilities.rank_zero][INFO] - TPU available: False, using: 0 TPU cores
[2024-07-16 19:31:39,328][pytorch_lightning.utilities.rank_zero][INFO] - IPU available: False, using: 0 IPUs
[2024-07-16 19:31:39,329][pytorch_lightning.utilities.rank_zero][INFO] - HPU available: False, using: 0 HPUs
[2024-07-16 19:31:39,330][src.train][INFO] - Logging hyperparameters!
C:\ProgramData\Anaconda3\envs\my_env\lib\site-packages\pytorch_lightning\callbacks\model_checkpoint.py:611: UserWarning: Checkpoint directory checkpoints/ exists and is not empty.
  rank_zero_warn(f"Checkpoint directory {dirpath} exists and is not empty.")
[2024-07-16 19:32:08,352][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2024-07-16 19:32:08,360][pytorch_lightning.callbacks.model_summary][INFO] -
  | Name      | Type              | Params
------------------------------------------------
0 | encoder   | SparK_2D_encoder  | 24.6 M
1 | diffusion | GaussianDiffusion | 44.1 M
------------------------------------------------
68.6 M    Trainable params
0         Non-trainable params
68.6 M    Total params
137.239   Total estimated model params size (MB)
Epoch 2: 100%|█████████████████████████████████████████████████| 14/14 [02:59<00:00, 12.84s/it, loss=0.204, v_num=gg30]
[2024-07-16 19:44:31,404][src.train][INFO] - Best checkpoint path:
checkpoints/epoch-2_step-36_loss-0.00_fold-1.ckpt
[2024-07-16 19:44:31,405][src.train][INFO] - Best checkpoint metric:
0.19755272567272186
[2024-07-16 19:44:31,406][src.train][INFO] - Starting evaluation phase of fold 1!
[2024-07-16 19:44:31,406][src.train][INFO] - Instantiating datamodule <src.datamodules.Datamodules_eval.Brats21>
[2024-07-16 19:47:07,228][src.train][INFO] - Validation of Datamodules_eval.Brats21!
[2024-07-16 19:47:07,230][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at checkpoints/epoch-2_step-36_loss-0.00_fold-1.ckpt
[2024-07-16 19:47:07,941][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2024-07-16 19:47:08,245][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from checkpoint at checkpoints/epoch-2_step-36_loss-0.00_fold-1.ckpt
Testing DataLoader 0: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [04:43<00:00,  2.83s/it]
[2024-07-16 19:52:26,120][src.train][INFO] - Test of Datamodules_eval.Brats21!
[2024-07-16 19:52:26,123][pytorch_lightning.utilities.rank_zero][INFO] - Restoring states from the checkpoint path at checkpoints/epoch-2_step-36_loss-0.00_fold-1.ckpt
[2024-07-16 19:52:26,881][pytorch_lightning.accelerators.gpu][INFO] - LOCAL_RANK: 0 - CUDA_VISIBLE_DEVICES: [0]
[2024-07-16 19:52:27,105][pytorch_lightning.utilities.rank_zero][INFO] - Loaded model weights from checkpoint at checkpoints/epoch-2_step-36_loss-0.00_fold-1.ckpt
Testing DataLoader 0:  11%|███████████████████▏                                                                                                                                                     | 131/1151 [05:13<40:17,  2.37s/it]



