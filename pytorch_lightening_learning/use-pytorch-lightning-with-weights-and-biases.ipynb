{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.9","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"This Kernel is based on this amazing [⚡Plant2021 PyTorch Lightning Starter [ Training ]⚡](https://www.kaggle.com/pegasos/plant2021-pytorch-lightning-starter-training) by [Sh1r0](https://www.kaggle.com/pegasos). This kernel is intended to showcase [Weights and Biases](https://wandb.ai/site) integration with PyTorch Lightning. \n\n# ⚡ PyTorch Lightning\n\nPyTorch is an extremely powerful framework for your deep learning research. But once the research gets complicated and things like 16-bit precision, multi-GPU training, and TPU training get mixed in, users are likely to introduce bugs. **PyTorch Lightning lets you decouple research from engineering.**\n\n**PyTorch Lightning ⚡ is not another framework but a style guide for PyTorch.**\n\nTo learn more about PyTorch Lightning check out my blog posts at Weights and Biases [Fully Connected](https://wandb.ai/fully-connected):\n\n* [Image Classification using PyTorch Lightning](https://wandb.ai/wandb/wandb-lightning/reports/Image-Classification-using-PyTorch-Lightning--VmlldzoyODk1NzY)\n* [Transfer Learning Using PyTorch Lightning](https://wandb.ai/wandb/wandb-lightning/reports/Transfer-Learning-Using-PyTorch-Lightning--VmlldzoyODk2MjA)\n* [Multi-GPU Training Using PyTorch Lightning](https://wandb.ai/wandb/wandb-lightning/reports/Multi-GPU-Training-Using-PyTorch-Lightning--VmlldzozMTk3NTk)\n\n# <img src=\"https://i.imgur.com/gb6B4ig.png\" width=\"400\" alt=\"Weights & Biases\" />\n\nWeights & Biases helps you build better models faster with a central dashboard for your machine learning projects. It not only logs your training metrics but can log hyperparameters and output metrics, then visualize and compare results and quickly share findings with your team mates. Track everything you need to make your models reproducible with Weights & Biases— from hyperparameters and code to model weights and dataset versions. \n\n### [Check this Kaggle kernel to learn more about Weights and Biases$\\rightarrow$](https://www.kaggle.com/ayuraj/experiment-tracking-with-weights-and-biases)\n![img](https://i.imgur.com/BGgfZj3.png)\n\n# PyTorch Lightning + Weights and Biases \n\nPyTorch Lightning provides a lightweight wrapper for organizing your PyTorch code and easily adding advanced features such as distributed training and 16-bit precision. W&B provides a lightweight wrapper for logging your ML experiments. It is incorporated directly into the PyTorch Lightning library, so you can check out [their documentation](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger) for the API and reference info.\n\n### Use the intergration in few lines of code.\n\n```\nfrom pytorch_lightning.loggers import WandbLogger  # newline 1\nfrom pytorch_lightning import Trainer\n\nwandb_logger = WandbLogger()  # newline 2\ntrainer = Trainer(logger=wandb_logger)\n```\n\n[![thumbnail](https://i.imgur.com/M7xZ04g.png)](https://www.youtube.com/watch?v=hUXQm46TAKc)\n","metadata":{}},{"cell_type":"markdown","source":"# 🧰 Imports and Setups","metadata":{}},{"cell_type":"code","source":"!pip install --upgrade -q wandb\n\n# Install timm \n!pip install -q timm","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"markdown","source":"## Import WandbLogger\n\nCoupled with [Weights & Biases integration](https://docs.wandb.com/library/integrations/lightning), you can quickly train and monitor models for full traceability and reproducibility with only 2 extra lines of code:\n\n```python\nfrom pytorch_lightning.loggers import WandbLogger\nwandb_logger = WandbLogger()\n```\nCheck out the documentation [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger).\n","metadata":{}},{"cell_type":"code","source":"import wandb\nfrom pytorch_lightning.loggers import WandbLogger\n\nwandb.login()","metadata":{"trusted":true},"execution_count":2,"outputs":[{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n","output_type":"stream"},{"output_type":"stream","name":"stdin","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter:  ········································\n"},{"name":"stderr","text":"\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n","output_type":"stream"},{"execution_count":2,"output_type":"execute_result","data":{"text/plain":"True"},"metadata":{}}]},{"cell_type":"code","source":"import cv2\nimport timm\nimport torch\nimport numpy as np\nimport pandas as pd\n\nimport torch.nn as nn\nimport albumentations as A\nimport pytorch_lightning as pl\nimport matplotlib.pyplot as plt\n\nfrom torch.utils.data import Dataset, DataLoader\nfrom albumentations.core.composition import Compose, OneOf\nfrom albumentations.augmentations.transforms import CLAHE, GaussNoise, ISONoise\nfrom albumentations.pytorch import ToTensorV2\n\nfrom pytorch_lightning import Trainer, seed_everything\nfrom pytorch_lightning import Callback\nfrom pytorch_lightning.loggers import CSVLogger\nfrom pytorch_lightning.callbacks import ModelCheckpoint, EarlyStopping\n\nfrom sklearn.model_selection import train_test_split","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"# 📀 Hyperparameters","metadata":{}},{"cell_type":"code","source":"# Config dictionary that will be logged to W&B.\nCONFIG = dict (\n    seed = 42,\n    train_val_split = 0.2,\n    model_name = 'resnet50',\n    pretrained = True,\n    img_size = 256,\n    num_classes = 12,\n    lr = 5e-4,\n    min_lr = 1e-6,\n    t_max = 20,\n    num_epochs = 10,\n    batch_size = 32,\n    accum = 1,\n    precision = 16,\n    n_fold = 5,\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n)\n\n# Directories\nPATH = \"../input/plant-pathology-2021-fgvc8/\"\n\nimage_size = CONFIG['img_size']\nTRAIN_DIR = f'../input/resized-plant2021/img_sz_{image_size}/'\nTEST_DIR = PATH + 'test_images/'\n\n# Seed everything\nseed_everything(CONFIG['seed'])","metadata":{"trusted":true},"execution_count":4,"outputs":[{"name":"stderr","text":"Global seed set to 42\n","output_type":"stream"},{"execution_count":4,"output_type":"execute_result","data":{"text/plain":"42"},"metadata":{}}]},{"cell_type":"markdown","source":"# 🔧 DataModule","metadata":{}},{"cell_type":"code","source":"# Read CSV file\ndf = pd.read_csv(PATH + \"train.csv\")\n\n# Label encode \nlabels = list(df['labels'].value_counts().keys())\nlabels_dict = dict(zip(labels, range(12)))\ndf = df.replace({\"labels\": labels_dict})\ndf.head()","metadata":{"trusted":true},"execution_count":5,"outputs":[{"execution_count":5,"output_type":"execute_result","data":{"text/plain":"                  image  labels\n0  800113bb65efe69e.jpg       1\n1  8002cb321f8bfcdf.jpg       7\n2  80070f7fb5e2ccaa.jpg       0\n3  80077517781fb94f.jpg       0\n4  800cbf0ff87721f8.jpg       4","text/html":"<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>image</th>\n      <th>labels</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>800113bb65efe69e.jpg</td>\n      <td>1</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>8002cb321f8bfcdf.jpg</td>\n      <td>7</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>80070f7fb5e2ccaa.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>80077517781fb94f.jpg</td>\n      <td>0</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>800cbf0ff87721f8.jpg</td>\n      <td>4</td>\n    </tr>\n  </tbody>\n</table>\n</div>"},"metadata":{}}]},{"cell_type":"code","source":"class PlantDataset(Dataset):\n    def __init__(self, df, transform=None):\n        self.image_id = df['image'].values\n        self.labels = df['labels'].values\n        self.transform = transform\n\n    def __len__(self):\n        return len(self.labels)\n\n    def __getitem__(self, idx):\n        image_id = self.image_id[idx]\n        label = self.labels[idx]\n        \n        image_path = TRAIN_DIR + image_id\n        image = cv2.imread(image_path)\n        image = cv2.cvtColor(image, cv2.COLOR_BGR2RGB)\n        \n        augmented = self.transform(image=image)\n        image = augmented['image']\n        return {'image':image, 'target': label}","metadata":{"trusted":true},"execution_count":6,"outputs":[]},{"cell_type":"code","source":"class PlantDataModule(pl.LightningDataModule):\n    def __init__(self, batch_size, data_dir: str = './'):\n        super().__init__()\n        self.batch_size = batch_size\n        \n        # Train augmentation policy\n        self.train_transform = Compose([\n            A.RandomResizedCrop(height=CONFIG['img_size'], width=CONFIG['img_size']),\n            A.HorizontalFlip(p=0.5),\n            A.ShiftScaleRotate(p=0.5),\n            A.RandomBrightnessContrast(p=0.5),\n            A.Normalize(),\n            ToTensorV2(),\n        ])\n\n        # Validation/Test augmentation policy\n        self.test_transform = Compose([\n            A.Resize(height=CONFIG['img_size'], width=CONFIG['img_size']),\n            A.Normalize(),\n            ToTensorV2(),\n        ])\n        \n\n    def setup(self, stage=None):\n        # Assign train/val datasets for use in dataloaders\n        if stage == 'fit' or stage is None:\n            # Random train-validation split\n            train_df, valid_df = train_test_split(df, test_size=CONFIG['train_val_split'])\n            \n            # Train dataset\n            self.train_dataset = PlantDataset(train_df, self.train_transform)\n            # Validation dataset\n            self.valid_dataset = PlantDataset(valid_df, self.test_transform)\n                        \n    def train_dataloader(self):\n        return DataLoader(self.train_dataset, batch_size=self.batch_size, shuffle=True, num_workers=4, drop_last=True)\n\n    def val_dataloader(self):\n        return DataLoader(self.valid_dataset, batch_size=self.batch_size, num_workers=4)","metadata":{"trusted":true},"execution_count":7,"outputs":[]},{"cell_type":"markdown","source":"# 🎺 LightningModule - Define the System","metadata":{}},{"cell_type":"code","source":"class CustomResNet(nn.Module):\n    def __init__(self, model_name='resnet18', pretrained=False):\n        super().__init__()\n        self.model = timm.create_model(model_name, pretrained=pretrained)\n        in_features = self.model.get_classifier().in_features\n        self.model.fc = nn.Linear(in_features, CONFIG['num_classes'])\n\n    def forward(self, x):\n        x = self.model(x)\n        return x","metadata":{"trusted":true},"execution_count":8,"outputs":[]},{"cell_type":"code","source":"class LitCassava(pl.LightningModule):\n    def __init__(self, model):\n        super(LitCassava, self).__init__()\n        self.model = model\n        self.metric = pl.metrics.F1(num_classes=CONFIG['num_classes'])\n        self.criterion = nn.CrossEntropyLoss()\n        self.lr = CONFIG['lr']\n\n    def forward(self, x, *args, **kwargs):\n        return self.model(x)\n\n    def configure_optimizers(self):\n        self.optimizer = torch.optim.Adam(self.model.parameters(), lr=self.lr)\n        self.scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(self.optimizer, T_max=CONFIG['t_max'], eta_min=CONFIG['min_lr'])\n\n        return {'optimizer': self.optimizer, 'lr_scheduler': self.scheduler}\n\n    def training_step(self, batch, batch_idx):\n        image = batch['image']\n        target = batch['target']\n        output = self.model(image)\n        loss = self.criterion(output, target)\n        score = self.metric(output.argmax(1), target)\n        logs = {'train_loss': loss, 'train_f1': score, 'lr': self.optimizer.param_groups[0]['lr']}\n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss\n\n    def validation_step(self, batch, batch_idx):\n        image = batch['image']\n        target = batch['target']\n        output = self.model(image)\n        loss = self.criterion(output, target)\n        score = self.metric(output.argmax(1), target)\n        logs = {'valid_loss': loss, 'valid_f1': score}\n        self.log_dict(\n            logs,\n            on_step=False, on_epoch=True, prog_bar=True, logger=True\n        )\n        return loss","metadata":{"trusted":true},"execution_count":9,"outputs":[]},{"cell_type":"markdown","source":"# 📲 Callbacks\n","metadata":{}},{"cell_type":"code","source":"# Checkpoint\ncheckpoint_callback = ModelCheckpoint(monitor='valid_loss',\n                                      save_top_k=1,\n                                      save_last=True,\n                                      save_weights_only=True,\n                                      filename='checkpoint/{epoch:02d}-{valid_loss:.4f}-{valid_f1:.4f}',\n                                      verbose=False,\n                                      mode='min')\n\n# Earlystopping\nearlystopping = EarlyStopping(monitor='valid_loss', patience=3, mode='min')","metadata":{"trusted":true},"execution_count":10,"outputs":[]},{"cell_type":"code","source":"# Custom Callback\nclass ImagePredictionLogger(Callback):\n    def __init__(self, val_samples, num_samples=32):\n        super().__init__()\n        self.num_samples = num_samples\n        self.val_imgs, self.val_labels = val_samples['image'], val_samples['target']\n        \n    def on_validation_epoch_end(self, trainer, pl_module):\n        # Bring the tensors to CPU\n        val_imgs = self.val_imgs.to(device=pl_module.device)\n        val_labels = self.val_labels.to(device=pl_module.device)\n        # Get model prediction\n        logits = pl_module(val_imgs)\n        preds = torch.argmax(logits, -1)\n        # Log the images as wandb Image\n        trainer.logger.experiment.log({\n            \"examples\":[wandb.Image(x, caption=f\"Pred:{pred}, Label:{y}\") \n                           for x, pred, y in zip(val_imgs[:self.num_samples], \n                                                 preds[:self.num_samples], \n                                                 val_labels[:self.num_samples])]\n            }, commit=False)","metadata":{"trusted":true},"execution_count":11,"outputs":[]},{"cell_type":"markdown","source":"> 📌 Tip: When logging manually through `wandb.log` or `trainer.logger.experiment.log`, make sure to use `commit=False` so the logging step does not increase.","metadata":{}},{"cell_type":"markdown","source":"## ⚡ Train and Evaluate the Model with W&B\n","metadata":{}},{"cell_type":"code","source":"# Init our data pipeline\ndatamodule = PlantDataModule(batch_size=CONFIG['batch_size'])\ndatamodule.setup()\n\n# Samples required by the custom ImagePredictionLogger callback to log image predictions.\nval_samples = next(iter(datamodule.val_dataloader()))\nval_imgs, val_labels = val_samples['image'], val_samples['target']\nval_imgs.shape, val_labels.shape","metadata":{"trusted":true},"execution_count":12,"outputs":[{"execution_count":12,"output_type":"execute_result","data":{"text/plain":"(torch.Size([32, 3, 256, 256]), torch.Size([32]))"},"metadata":{}}]},{"cell_type":"code","source":"# Init our model\nmodel = CustomResNet(model_name=CONFIG['model_name'], pretrained=CONFIG['pretrained'])\nlit_model = LitCassava(model)","metadata":{"trusted":true},"execution_count":13,"outputs":[{"name":"stderr","text":"Downloading: \"https://github.com/rwightman/pytorch-image-models/releases/download/v0.1-weights/resnet50_ram-a26f946b.pth\" to /root/.cache/torch/hub/checkpoints/resnet50_ram-a26f946b.pth\n","output_type":"stream"}]},{"cell_type":"markdown","source":"Check out the documentation for WandbLogger [here](https://pytorch-lightning.readthedocs.io/en/stable/extensions/generated/pytorch_lightning.loggers.WandbLogger.html#pytorch_lightning.loggers.WandbLogger).\n\n> 📌 Tip: dditional arguments like entity, group, tags, etc. used by `wandb.init()` can be passed as keyword arguments in this logger.","metadata":{}},{"cell_type":"code","source":"## Initialize wandb logger\nwandb_logger = WandbLogger(project='plant-pathology-lightning', \n                           config=CONFIG,\n                           group='ResNet', \n                           job_type='train')\n\n# Initialize a trainer\ntrainer = Trainer(\n            max_epochs=CONFIG['num_epochs'],\n            gpus=1,\n            accumulate_grad_batches=CONFIG['accum'],\n            precision=CONFIG['precision'],\n            callbacks=[earlystopping,\n                       ImagePredictionLogger(val_samples)],\n            checkpoint_callback=checkpoint_callback,\n            logger=wandb_logger,\n            weights_summary='top',\n)\n\n# Train the model ⚡🚅⚡\ntrainer.fit(lit_model, datamodule)\n\n# Close wandb run\nwandb.finish() ","metadata":{"trusted":true},"execution_count":14,"outputs":[{"name":"stderr","text":"GPU available: True, used: True\nTPU available: None, using: 0 TPU cores\nUsing native 16bit precision.\n\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mayush-thakur\u001b[0m (use `wandb login --relogin` to force relogin)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                Tracking run with wandb version 0.10.28<br/>\n                Syncing run <strong style=\"color:#cdcd00\">cool-frog-7</strong> to <a href=\"https://wandb.ai\" target=\"_blank\">Weights & Biases</a> <a href=\"https://docs.wandb.com/integrations/jupyter.html\" target=\"_blank\">(Documentation)</a>.<br/>\n                Project page: <a href=\"https://wandb.ai/ayush-thakur/plant-pathology-lightning\" target=\"_blank\">https://wandb.ai/ayush-thakur/plant-pathology-lightning</a><br/>\n                Run page: <a href=\"https://wandb.ai/ayush-thakur/plant-pathology-lightning/runs/11c7pitj\" target=\"_blank\">https://wandb.ai/ayush-thakur/plant-pathology-lightning/runs/11c7pitj</a><br/>\n                Run data is saved locally in <code>/kaggle/working/wandb/run-20210430_182413-11c7pitj</code><br/><br/>\n            "},"metadata":{}},{"name":"stderr","text":"\n  | Name      | Type             | Params\n-----------------------------------------------\n0 | model     | CustomResNet     | 23.5 M\n1 | metric    | F1               | 0     \n2 | criterion | CrossEntropyLoss | 0     \n-----------------------------------------------\n23.5 M    Trainable params\n0         Non-trainable params\n23.5 M    Total params\n94.130    Total estimated model params size (MB)\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"Validation sanity check: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Training: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":"b8de61f3054548598b63f90a80b6f493"}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"Validating: |          | 0/? [00:00<?, ?it/s]","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"name":"stderr","text":"Saving latest checkpoint...\n","output_type":"stream"},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<br/>Waiting for W&B process to finish, PID 132<br/>Program ended successfully."},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"VBox(children=(Label(value=' 31.51MB of 31.68MB uploaded (0.00MB deduped)\\r'), FloatProgress(value=0.994544584…","application/vnd.jupyter.widget-view+json":{"version_major":2,"version_minor":0,"model_id":""}},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find user logs for this run at: <code>/kaggle/working/wandb/run-20210430_182413-11c7pitj/logs/debug.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Find internal logs for this run at: <code>/kaggle/working/wandb/run-20210430_182413-11c7pitj/logs/debug-internal.log</code>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run summary:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>1353</td></tr><tr><td>_timestamp</td><td>1619808406</td></tr><tr><td>_step</td><td>4649</td></tr><tr><td>train_loss</td><td>0.64505</td></tr><tr><td>train_f1</td><td>0.79147</td></tr><tr><td>lr</td><td>0.00029</td></tr><tr><td>epoch</td><td>9</td></tr><tr><td>valid_loss</td><td>0.45967</td></tr><tr><td>valid_f1</td><td>0.86021</td></tr></table>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"<h3>Run history:</h3><br/><style>\n    table.wandb td:nth-child(1) { padding: 0 10px; text-align: right }\n    </style><table class=\"wandb\">\n<tr><td>_runtime</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_timestamp</td><td>▁▂▃▃▄▅▆▆▇██</td></tr><tr><td>_step</td><td>▁▂▂▃▄▄▅▆▇▇█</td></tr><tr><td>train_loss</td><td>█▅▄▃▃▂▂▂▁▁</td></tr><tr><td>train_f1</td><td>▁▄▅▆▆▇▇▇██</td></tr><tr><td>lr</td><td>███▇▆▆▅▃▂▁</td></tr><tr><td>epoch</td><td>▁▂▃▃▄▅▆▆▇█</td></tr><tr><td>valid_loss</td><td>██▆▅▅▅▄▃▁▁</td></tr><tr><td>valid_f1</td><td>▁▁▃▄▃▃▄▆██</td></tr></table><br/>"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"Synced 5 W&B file(s), 352 media file(s), 0 artifact file(s) and 2 other file(s)"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"<IPython.core.display.HTML object>","text/html":"\n                    <br/>Synced <strong style=\"color:#cdcd00\">cool-frog-7</strong>: <a href=\"https://wandb.ai/ayush-thakur/plant-pathology-lightning/runs/11c7pitj\" target=\"_blank\">https://wandb.ai/ayush-thakur/plant-pathology-lightning/runs/11c7pitj</a><br/>\n                "},"metadata":{}}]},{"cell_type":"markdown","source":"## Visualize Metrics\n\n![img](https://i.imgur.com/n6P7K4M.gif)\n\n## Visualize Model Predictions\n\n![img](https://i.imgur.com/lgkLnrt.gif)\n\n## Visualize CPU and GPU Metrics\n\n![img](https://i.imgur.com/ZLjrbhj.gif)\n\n# ❄️ Resources\n\nI hope you find this kernel useful and will encouage you to try out Weights and Biases. Here are some relevant links that you might want to check out:\n\n* Check out the [official documentation](https://docs.wandb.ai/) to learn more about the best practices and advanced features. \n\n* Check out the [examples GitHub repository](https://github.com/wandb/examples) for curated and minimal examples. This can be a good starting point. \n\n* [Weights and Biases Fully Connected](https://wandb.ai/fully-connected) is a home for curated tutorials, free-form dicussions, paper summaries, industry expert advices and more. ","metadata":{}}]}